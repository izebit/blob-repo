{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "KU56zkC0S80M"
   },
   "outputs": [],
   "source": [
    "%%writefile mapper-1.py\n",
    "\n",
    "\n",
    "import sys\n",
    "import re\n",
    "\n",
    "\n",
    "reload(sys)\n",
    "sys.setdefaultencoding('utf-8')  # required to convert to unicode\n",
    "\n",
    "path = 'stop_words_en.txt'\n",
    "stop_words = set()\n",
    "\n",
    "# Your code for reading stop words here\n",
    "for l in open(path):\n",
    "    line = l.strip()\n",
    "    stop_words.add(line.strip())\n",
    "\n",
    "\n",
    "for line in sys.stdin:\n",
    "    try:\n",
    "        article_id, text = unicode(line.strip()).split('\\t', 1)\n",
    "    except ValueError as e:\n",
    "        continue\n",
    "        \n",
    "    text = re.sub(\"^\\W+|\\W+$\", \"\", text, flags=re.UNICODE)\n",
    "    words = re.split(\"\\W*\\s+\\W*\", text, flags=re.UNICODE)\n",
    "\n",
    "    total_count = 0\n",
    "    unique_words = set()\n",
    "    for w in words:\n",
    "        word = w.lower().strip()\n",
    "        if word not in stop_words:\n",
    "            print(\"%s\\t%s\\t%s\" % (article_id, word, 1))\n",
    "            print(\"%s\\t%s\\t%s\" % (article_id, \" \", 1))\n",
    "            \n",
    "            if word not in unique_words:\n",
    "                print(\"%s\\t%s\\t%s\" % (\" \", word, 1))\n",
    "                unique_words.add(word)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "yslvpwpfS80P"
   },
   "outputs": [],
   "source": [
    "%%writefile reducer-1.py\n",
    "\n",
    "import sys\n",
    "import re\n",
    "\n",
    "\n",
    "reload(sys)\n",
    "sys.setdefaultencoding('utf-8')  # required to convert to unicode\n",
    "\n",
    "\n",
    "current_article_id = None\n",
    "current_word = None\n",
    "current_count = 0\n",
    "\n",
    "for line in sys.stdin:\n",
    "    try:\n",
    "        article_id, word, count = unicode(line).split('\\t', 2)\n",
    "    except ValueError as e:\n",
    "        raise e\n",
    "                        \n",
    "    if article_id == current_article_id and word == current_word:\n",
    "        current_count = int(count) + current_count \n",
    "    else:\n",
    "        if current_article_id:\n",
    "            print(\"%s\\t%s\\t%s\" % (current_article_id, current_word, current_count))\n",
    "            \n",
    "        current_article_id = article_id\n",
    "        current_word = word\n",
    "        current_count = int(count)\n",
    "\n",
    "if current_count > 0:\n",
    "    print(\"%s\\t%s\\t%s\" % (current_article_id, current_word, current_count))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "OUT_DIR=\"intermediate-result-1\"\n",
    "hdfs dfs -rm -r ${OUT_DIR} > /dev/null\n",
    "\n",
    "\n",
    "yarn jar /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce/hadoop-streaming.jar \\\n",
    "    -D mapred.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "    -D mapred.jab.name=\"tf-idf-job\" \\\n",
    "    -D mapreduce.job.reduces=5 \\\n",
    "    -D stream.num.map.output.key.fields=2 \\\n",
    "    -D mapred.text.key.partitioner.options=\"-k1,1\" \\\n",
    "    -D mapred.text.key.comparator.options=\"-k1,1 -k2,2\" \\\n",
    "    -files mapper-1.py,reducer-1.py,/datasets/stop_words_en.txt \\\n",
    "    -mapper \"python2 mapper-1.py\" \\\n",
    "    -reducer \"python2 reducer-1.py\" \\\n",
    "    -partitioner org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner \\\n",
    "    -input /data/wiki/en_articles_part \\\n",
    "    -output ${OUT_DIR} > /dev/null\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile reducer-2.py\n",
    "\n",
    "import sys\n",
    "import re\n",
    "\n",
    "\n",
    "reload(sys)\n",
    "sys.setdefaultencoding('utf-8')  # required to convert to unicode\n",
    "\n",
    "current_article_id = None\n",
    "count_terms_in_article = 0\n",
    "current_word = None\n",
    "\n",
    "\n",
    "for line in sys.stdin:\n",
    "    try:\n",
    "        article_id, word, count = unicode(line).split('\\t', 2)\n",
    "    except ValueError as e:\n",
    "        print(line)\n",
    "        continue\n",
    "                \n",
    "    if article_id == \" \":\n",
    "        print(\"%s\\t%s\\t%s\" % (word, article_id, int(count)))\n",
    "    elif current_article_id == article_id:\n",
    "        tf = float(count)/count_terms_in_article\n",
    "        print(\"%s\\t%s\\t%s\" % (word, article_id, tf))\n",
    "    else:\n",
    "        current_article_id = article_id\n",
    "        count_terms_in_article = float(count)\n",
    "        current_word = word\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile identity-mapper.py\n",
    "\n",
    "import sys\n",
    "\n",
    "reload(sys)\n",
    "sys.setdefaultencoding('utf-8')  # required to convert to unicode\n",
    "\n",
    "\n",
    "for line in sys.stdin:\n",
    "    print(unicode(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "IN_DIR=\"intermediate-result-1\"\n",
    "OUT_DIR=\"intermediate-result-2\"\n",
    "hdfs dfs -rm -r ${OUT_DIR} > /dev/null\n",
    "\n",
    "\n",
    "yarn jar /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce/hadoop-streaming.jar \\\n",
    "    -D mapred.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "    -D mapred.jab.name=\"tf-idf-job-2\" \\\n",
    "    -D mapreduce.job.reduces=5 \\\n",
    "    -D stream.num.map.output.key.fields=2 \\\n",
    "    -D mapred.text.key.partitioner.options=\"-k1,1\" \\\n",
    "    -D mapred.text.key.comparator.options=\"-k1,1 -k2,2\" \\\n",
    "    -files reducer-2.py,identity-mapper.py \\\n",
    "    -mapper \"python2 identity-mapper.py\" \\\n",
    "    -reducer \"python2 reducer-2.py\" \\\n",
    "    -partitioner org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner \\\n",
    "    -input ${IN_DIR} \\\n",
    "    -output ${OUT_DIR} > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile reducer-3.py\n",
    "\n",
    "import sys\n",
    "import re\n",
    "from math import log\n",
    "\n",
    "\n",
    "reload(sys)\n",
    "sys.setdefaultencoding('utf-8')  # required to convert to unicode\n",
    "\n",
    "count_of_articles_with_term = 0\n",
    "current_word = None\n",
    "\n",
    "\n",
    "for line in sys.stdin:\n",
    "    try:\n",
    "        word, article_id, num = unicode(line).split('\\t', 2)\n",
    "    except ValueError as e:\n",
    "        continue\n",
    "                        \n",
    "    if current_word == word:\n",
    "        idf = 1.0/log(1 + count_of_articles_with_term)\n",
    "        tf = float(num)\n",
    "        result = str(tf*idf)\n",
    "        print(\"%s\\t%s\\t%s\" % (word, article_id, result))\n",
    "    else:\n",
    "        count_of_articles_with_term = int(num)\n",
    "        current_word = word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "IN_DIR=\"intermediate-result-2\"\n",
    "OUT_DIR=\"result\"\n",
    "hdfs dfs -rm -r ${OUT_DIR} > /dev/null\n",
    "\n",
    "\n",
    "yarn jar /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce/hadoop-streaming.jar \\\n",
    "    -D mapred.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "    -D mapred.jab.name=\"tf-idf-job-3\" \\\n",
    "    -D mapreduce.job.reduces=5 \\\n",
    "    -D stream.num.map.output.key.fields=2 \\\n",
    "    -D mapred.text.key.partitioner.options=\"-k1,1\" \\\n",
    "    -D mapred.text.key.comparator.options=\"-k1,1 -k2,2\" \\\n",
    "    -files reducer-3.py,identity-mapper.py \\\n",
    "    -mapper \"python2 identity-mapper.py\" \\\n",
    "    -reducer \"python2 reducer-3.py\" \\\n",
    "    -partitioner org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner \\\n",
    "    -input ${IN_DIR} \\\n",
    "    -output ${OUT_DIR} > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -cat intermediate-result-3/* | grep -P 'labor\\t12' | head -1 | awk '{print $3}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "default_view": {},
   "name": "702_to_students.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
